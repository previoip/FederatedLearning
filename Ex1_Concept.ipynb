{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Categorical Structured Data ML with Keras\n",
        "\n",
        "[Link to Colab (deprecated)](https://colab.research.google.com/drive/1GmAhxnKVvrhWffospDEe0rc-QB_tjfhE?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh86Sf02XCul"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "from src.custom_types import TypeEnum\n",
        "from src.tf_layer_constructors import (\n",
        "    gen_normalization_layer,\n",
        "    gen_multihot_categorical_encoding_layer\n",
        ")\n",
        "from src.tf_utils import df_to_tfds\n",
        "from src.data_examples.ex1_data_loader import ExampleDataLoader\n",
        "\n",
        "print('Using TensorFlow version', tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RAND_SEED = 1337\n",
        "\n",
        "np.random.seed(RAND_SEED)\n",
        "random.seed(RAND_SEED)\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_model_path = Path('saved_models')\n",
        "saved_model_path.mkdir(exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Example Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = ExampleDataLoader()\n",
        "\n",
        "data.download().load().clean()\n",
        "data.df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.df['symboling_threshold'] = [1 if i > 0 else 0 for i in data.df['symboling']]\n",
        "\n",
        "target_feature_label = 'symboling_threshold'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.pairplot(\n",
        "  data.df[[\n",
        "    data.feature_label,\n",
        "    \"curb_weight\",\n",
        "    \"engine_size\",\n",
        "    \"horsepower\",\n",
        "    # \"peak_rpm\",\n",
        "    \"city_mpg\",\n",
        "    \"highway_mpg\",\n",
        "    \"price\"\n",
        "  ]], diag_kind='kde', hue=data.feature_label, palette=sns.color_palette('hls', len(data.df[data.feature_label].unique())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_random_sample_from_spec(data_spec, features_override=[]):\n",
        "  ret = {}\n",
        "  for k in data_spec.keys():\n",
        "    if features_override and k not in features_override:\n",
        "      continue\n",
        "    v = data_spec.get(k)\n",
        "    if isinstance(v, tuple):\n",
        "      ret[k] = random.random() * (v[1] - v[0])\n",
        "    elif isinstance(v, list):\n",
        "      ret[k] = random.choice(v)\n",
        "    else:\n",
        "      ret[k] = v\n",
        "  return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "__inference_sample_spec = list(map(lambda x: x.replace('_', '-'), data.features_categorical + data.features_numeric_continuous))\n",
        "__inference_sample = generate_random_sample_from_spec(data.data_spec, __inference_sample_spec)\n",
        "inference_sample = {}\n",
        "for k, v in __inference_sample.items():\n",
        "  inference_sample[k.replace('-', '_')] = tf.convert_to_tensor([v])\n",
        "\n",
        "inference_sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Centralized (Conventional) Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epoch = 25\n",
        "batch_size = 24\n",
        "\n",
        "ex1ch1_model_path = saved_model_path / 'ex1ch1_auto_classifier'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = data.df.sample(frac=0.8, random_state=RAND_SEED)\n",
        "df_val_test = data.df.drop(df_train.index)\n",
        "df_test = df_val_test.sample(frac=0.5, random_state=RAND_SEED)\n",
        "df_val = df_val_test.drop(df_test.index)\n",
        "\n",
        "tfds_train  = df_to_tfds(df_train, target_feature_label, batch_size=batch_size, )\n",
        "tfds_test   = df_to_tfds(df_test,  target_feature_label, batch_size=batch_size, )\n",
        "tfds_val    = df_to_tfds(df_val,   target_feature_label, batch_size=batch_size, )\n",
        "\n",
        "df_train.shape, df_test.shape, df_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_inputs = {}\n",
        "all_encoded_features = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_inputs['normalization'] = {}\n",
        "all_encoded_features['normalization'] = {}\n",
        "\n",
        "for col_name in data.features_numeric_continuous:\n",
        "\n",
        "  input_numeric = tf.keras.Input(shape=(1,), name=col_name, dtype='float32')\n",
        "  normalization_layer = gen_normalization_layer(tfds_train, col_name)\n",
        "  encoded_normalized_input = normalization_layer(input_numeric)\n",
        "\n",
        "  all_inputs['normalization'][col_name] = input_numeric\n",
        "  all_encoded_features['normalization'][col_name] = encoded_normalized_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_inputs['categorical'] = {}\n",
        "all_encoded_features['categorical'] = {}\n",
        "\n",
        "for col_name in data.features_categorical:\n",
        "  input_categorical = tf.keras.Input(shape=(1,), name=col_name, dtype='string')\n",
        "  categorical_encoder = gen_multihot_categorical_encoding_layer(tfds_train, col_name, TypeEnum.string, max_tokens=5)\n",
        "  encoded_categorical_input = categorical_encoder(input_categorical)\n",
        "\n",
        "  all_inputs['categorical'][col_name] = input_categorical\n",
        "  all_encoded_features['categorical'][col_name] = encoded_categorical_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_categorical_model(input_layers, feature_layers):\n",
        "  nn_dense = tf.keras.layers.Dense(32, activation='relu')(feature_layers)\n",
        "  nn_dense = tf.keras.layers.Dropout(0.5)(nn_dense)\n",
        "  output = tf.keras.layers.Dense(1)(nn_dense)\n",
        "  return tf.keras.Model(input_layers, output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_inputs_layers = [j for i in all_inputs.values() for j in i.values()]\n",
        "all_features = [j for i in all_encoded_features.values() for j in i.values()]\n",
        "all_feature_layers = tf.keras.layers.concatenate(all_features)\n",
        "\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(all_feature_layers)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "output = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "model = tf.keras.Model(all_inputs_layers, output)\n",
        "all_encoded_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "class MetricsLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, filename, _metrics=['accuracy', 'loss']):\n",
        "        super().__init__()\n",
        "        self.filename = filename\n",
        "        self.file = None\n",
        "        self.writer = None\n",
        "        self._metrics=_metrics\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.file = open(self.filename, 'w')\n",
        "        self.writer = csv.DictWriter(self.file, ['epoch'] + self._metrics)\n",
        "        self.writer.writeheader()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(logs)\n",
        "        row = {'epoch': epoch + 1}\n",
        "        for k in self._metrics:\n",
        "            row[k] = logs.get(k, np.nan)\n",
        "        self.writer.writerow(row)\n",
        "        self.file.flush()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_logger = MetricsLogger('metrics.csv', ['loss', 'accuracy', 'val_loss', 'foo'])\n",
        "\n",
        "model.fit(tfds_train, epochs=n_epoch, validation_data=tfds_val, callbacks=[metrics_logger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(tfds_test)\n",
        "print(\"Accuracy\", accuracy)\n",
        "model.save(ex1ch1_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model(ex1ch1_model_path)\n",
        "predictions = loaded_model.predict(inference_sample)\n",
        "predictions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
